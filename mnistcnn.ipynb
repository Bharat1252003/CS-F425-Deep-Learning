{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Source: https://www.kaggle.com/code/cgurkan/mnist-with-pytorch-cnn/notebook","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport math\nimport copy\nimport time\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision.utils import make_grid\n\n#neural net imports\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n%matplotlib inline\nprint('Imports Ready.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_seed = 42\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(random_seed)\n\n# Load the data\ntrain_df = pd.read_csv(\"/kaggle/input/mnistdata/mnist_train_small.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/mnistdata/mnist_test.csv\")\n\ny = train_df.iloc[:,0]\nx = train_df.iloc[:,1:]\n\n#Split training data into Train and validation set\nX_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.15, shuffle=True)\n\nnum_epoch = 3\nbatch_size_train = 32\nbatch_size_test = 32\nlearning_rate = 0.002\nmomentum = 0.9\nlog_interval = 100\n\nprint('Hyperparameters loaded. Dataset loaded.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTDataset(Dataset):\n    def __init__(self,  data, target, train=True, transform=None):\n        \"\"\"\n        Args:\n            csv_path (string): path to csv file\n            transform: pytorch transforms for transforms and tensor conversion\n        \"\"\"\n        self.train = train\n        if self.train :\n            self.data = data\n            self.labels = np.asarray(target.iloc[:])\n        else:\n            self.data = data\n            self.labels = None\n        self.height = 28 # Height of image\n        self.width = 28 # Width of image\n        self.transform = transform\n\n    def __getitem__(self, index):\n        # Read each 784 pixels and reshape the 1D array ([784]) to 2D array ([28,28])\n        img_as_np = np.asarray(self.data.iloc[index][0:]).reshape(self.height, self.width).astype('uint8')\n        # Convert image from numpy array to PIL image, mode 'L' is for grayscale\n        img_as_img = Image.fromarray(img_as_np)\n        img_as_img = img_as_img.convert('L')\n        img_as_tensor = img_as_img\n        \n        if self.train:\n            single_image_label = self.labels[index]\n        else:\n            single_image_label = None\n            \n        # Transform image to tensor\n        if self.transform is not None:\n            img_as_tensor = self.transform(img_as_img)\n        \n        if self.train:\n        # Return image and the label                \n            return (img_as_tensor, single_image_label)\n        else:\n            return img_as_tensor\n    \n    def __len__(self):\n        return len(self.data.index)\n\nprint('MNISTDataset Class loaded.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.1310], std=[0.3085])])\ntrain = MNISTDataset(X_train, y_train, True, norm)\nvalid = MNISTDataset(X_valid, y_valid, True, norm)\ntest  = MNISTDataset(data=test_df, target=None, train=False, transform=norm)\n\ntrain_loader = DataLoader(train, batch_size=batch_size_train,num_workers=2, shuffle=True)\nvalid_loader = DataLoader(valid, batch_size=batch_size_test, num_workers=2, shuffle=True)\ntest_loader  = DataLoader(test,  batch_size=batch_size_test, shuffle=False)\nprint('Dataset loaded.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(images, nmax=64):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid((images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n    \ndef show_batch(dl, nmax=64):\n    for images in dl:\n        print(images[0].shape)\n        show_images(images[0], nmax=nmax)\n        print(images[1])\n        break\n\nshow_batch(train_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2) \n        )\n        \n        self.linear_block = nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Linear(128*7*7, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear_block(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model = Net()    \ncriterion = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    cnn_model.cuda()\n    criterion.cuda()\n\noptimizer = optim.Adam(params=cnn_model.parameters(), lr=learning_rate)    \n\nexp_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n\ntrain_losses = []\ntrain_counter = []\ntest_losses = []\ntest_counter = [i*len(train_loader.dataset) for i in range(1, num_epoch + 1)]    \n\nbest_model_wts = copy.deepcopy(cnn_model.state_dict())\nbest_acc = 0.0\n\nsince = time.time()\n\nfor epoch in range(1, num_epoch + 1):\n    cnn_model.train()    \n    for i, (images, labels) in enumerate(train_loader):\n        images = Variable(images)\n        labels = Variable(labels)\n        # Clear gradients\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = cnn_model(images)\n        # Calculate loss\n        loss = criterion(outputs, labels)\n        # Backward pass\n        loss.backward()\n        # Update weights\n        optimizer.step()\n        if (i + 1)% log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (i + 1) * len(images), len(train_loader.dataset),\n                100. * (i + 1) / len(train_loader), loss.data))\n            train_losses.append(loss.item())\n            train_counter.append((i*64) + ((epoch-1)*len(train_loader.dataset)))\n    cnn_model.eval()    \n    loss = 0    \n    running_corrects = 0\n    with torch.no_grad():       \n        for i, (data, target) in enumerate(valid_loader):\n            data = Variable(data)\n            target = Variable(target)\n            output = cnn_model(data)\n            loss += F.cross_entropy(output, target, reduction='sum').item()            \n            _, preds = torch.max(output, 1)            \n            running_corrects += torch.sum(preds == target.data)\n    loss /= len(valid_loader.dataset)\n    test_losses.append(loss)\n    epoch_acc = 100. * running_corrects.double() / len(valid_loader.dataset)\n    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n        loss, running_corrects, len(valid_loader.dataset), epoch_acc))\n    if epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(cnn_model.state_dict())\n    exp_lr_scheduler.step(loss)\n             \ntime_elapsed = time.time() - since\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\nprint('Best val Acc: {:4f}'.format(best_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.eval()\ntest_preds = None\ntest_preds = torch.LongTensor()\n\nfor i, data in enumerate(test_loader):\n    data = Variable(data)\n    output = cnn_model(data)\n    preds = output.cpu().data.max(1, keepdim=True)[1]\n    test_preds = torch.cat((test_preds, preds), dim=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}